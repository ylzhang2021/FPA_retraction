\documentclass{article}
\usepackage{amsmath,amsfonts,verbatim,amssymb}
\usepackage{multirow,diagbox}
\usepackage{float}
\usepackage{booktabs}
\usepackage{color}
\oddsidemargin 0in
\textwidth 6.5in
\textheight=9in
\topmargin=-0.5in

\allowdisplaybreaks[3]
\newcommand{\rrev}[1]{{\color{blue}#1}}

\def\R{\mathbb{R}}

\begin{document}



\section*{Author's responses to Referee 1}
Thank you very much for reading the manuscript carefully and providing useful suggestions. To facilitate the review process, we have marked all our changed in blue.  The following is our response to your suggestions and comments.

\paragraph{Main comments:}
\begin{enumerate}
	\item  \textit{\textbf{Reviewer:}} \textit{P3, L42: The closed ball with respect to which norm?}
	
	\textbf{Response:} Thanks for the comment. The closed ball is with respect to the Euclidean norm. We have clarifed it. See the beginning of Section 2.

	\item  \textit{\textbf{Reviewer:}} \textit{P4, L48-49: Please, give a reference for this assertion.}
	
	\textbf{Response:} Thanks for the comment. We have included a short deduction right after Definition 2.2.

	\item  \textit{\textbf{Reviewer:}} \textit{P5, Algorithm 1: It is not clear to me what is the role of parameter $\underline{\beta}$. Is it only used as a lower bound for the initial $\beta_k^0$? If so, how can you assert that $\inf_k \beta_k \ge \underline{\beta}$ in Remark 3.1? The same applies to Algorithm 2 and Remark 4.1.}
	
	\textbf{Response:} Thanks for the comment. Yes, $\underline{\beta}$ is used as a lower bound for the initial $\beta_k^0$. In Remark 3.1, we asserted that
\begin{equation}\label{infbeta}
\inf_k \beta_k \ge \beta_{\min}:= \textstyle\min\left\{\frac{\eta}{2}\left(\frac{c}2 + \frac{M_1L}{-2\max\limits_{1 \le i \le m}\{g_i(x^\odot)\}}\right)^{-1},\underline{\beta}\right\}.
\end{equation}
Indeed, from the proof of Theorem 3.1(iii), we see that the line search condition in Step 2b) will be satisfied as long as $\widetilde \beta \le \frac{1}{2}\left(\frac{c}2 + \frac{M_1L}{-2\max\limits_{1 \le i \le m}\{g_i(x^\odot)\}}\right)^{-1}$. By the definition of backtracking linesearch, if backtracking was invoked, it means that $\beta_k$ is accepted while $\eta^{-1}\beta_k$ was not accepted. Thus, it must hold that
\[
\eta^{-1}\beta_k > \frac{1}{2}\left(\frac{c}2 + \frac{M_1L}{-2\max\limits_{1 \le i \le m}\{g_i(x^\odot)\}}\right)^{-1}.
\]
Finally, we also need to consider the case that backtracking was not invoked at all: this happens when $\beta^0_k$ is already small enough. In this case, we make use of the lower bound $\underline{\beta}$ to conclude that $\beta_k \ge \underline{\beta}$. Overall, we get the lower bound as in \eqref{infbeta} above.
%
%In fact, if $\frac{\eta}{2}\left(\frac{c}2 + \frac{M_1L}{-2\max\limits_{1 \le i \le m}\{g_i(x^\odot)\}}\right)^{-1} > \underline{\beta}$, then $\beta_{min} = \underline{\beta}$, by the proof of Theorem~3.1(iii), we have the assert holds; if $\frac{\eta}{2}\left(\frac{c}2 + \frac{M_1L}{-2\max\limits_{1 \le i \le m}\{g_i(x^\odot)\}}\right)^{-1} < \underline{\beta}$, the assert is clearly valid. The same applies to Remark 4.1.

	\item  \textit{\textbf{Reviewer:}} \textit{P7, L23: I cannot see how [38, Th 2.6] is applied (same doubt in Theorem 4.1, P16 L9). Are you assuming full domain of functions P1 and P2? If so, make it more explicit.}
	
	\textbf{Response:} Thanks for the comment. We assume that $P_1:\R^n\rightarrow\R$ and $P_2:\R^n\rightarrow\R$ are continuous convex functions, which were assumed at the beginning of the article.

	\item  \textit{\textbf{Reviewer:}} \textit{P8, L43: Why is $\xi^k$ bounded? Maybe this it related with the full domain asked in item 4. The same for $\partial P_1(u^{k_j})$.}
	
	\textbf{Response:} Thanks for the comment. We assume that $P_1:\R^n\rightarrow\R$ and $P_2:\R^n\rightarrow\R$ are continuous convex functions at the beginning of the article.

	\item  \textit{\textbf{Reviewer:}} \textit{P10, L10: How does one get the term $+\frac{1}{\beta_k}\|u^k - x^k\|^2$? It is supposed that the authors are using the convexity of P1, but it is not strongly convex.}
	
	\textbf{Response:} Thanks for the comment. It only requires the convexity of $P_1$. In fact, it contains two steps. We first use the convexity of $P_1$, then invoke the nonnegative of $\frac{1}{2\beta_k}\|u^k - x^k\|^2$, i.e.,
\[
P_1(x^{k+1})\leq P_1(u^k) + \tau_k(P_1(x^\odot) - P_1(u^k))\leq P_1(u^k) + \tau_k(P_1(x^\odot) - P_1(u^k)) + \frac{1}{2\beta_k}\|u^k - x^k\|^2.
\]

	\item  \textit{\textbf{Reviewer:}} \textit{P16, L29: Which is the closed form formula for $\tilde{\tau}$?}
	
	\textbf{Response:} Thanks for the comment. By the definition of $\ell^{y}(u)$ in (4.4), we have that
\begin{equation*}
\begin{aligned}
&\ell^{x^k}(A((1-\widetilde\tau)\widetilde u + \widetilde\tau x^\circledcirc) - b) = \sum_{i=1}^p\varphi_+'\big((a_i^T x^k - b_i)^2\big)(A((1-\widetilde\tau)\widetilde u + \widetilde\tau x^\circledcirc) - b)_i^2\\
& = \sum_{i=1}^p\varphi_+'\big((a_i^T x^k - b_i)^2\big)((\widetilde\tau A(x^\circledcirc - \widetilde u))^2 + 2\widetilde\tau A(x^\circledcirc - \widetilde u)(A\widetilde u - b) + (A\widetilde u - b)^2)_i\\
&= \widetilde\tau^2(\sum_{i=1}^p\varphi_+'\big((a_i^T x^k - b_i)^2\big)(A(x^\circledcirc - \widetilde u))_i^2) \\
&~~~~+ 2\widetilde\tau (\sum_{i=1}^p\varphi_+'\big((a_i^T x^k - b_i)^2\big)(A(x^\circledcirc - \widetilde u)(A\widetilde u - b))_i) \\
&~~~~+ \sum_{i=1}^p\varphi_+'\big((a_i^T x^k - b_i)^2\big)(A\widetilde u - b)^2_i,
\end{aligned}
\end{equation*}
which is a quadratic function in $\widetilde\tau$. Therefore, from $\ell^{x^k}(A((1-\widetilde\tau)\widetilde u + \widetilde\tau x^\circledcirc) - b) = \tilde{\sigma}^{x^k}$, $\widetilde\tau$ admits a closed form formula.
	\item  \textit{\textbf{Reviewer:}} \textit{P23, L44: How is such $\tau$ found?}
	
	\textbf{Response:} Thanks for the comment. In fact,
\begin{equation*}
\begin{aligned}
\sigma^2 &= \|A(\widehat x_{\rm spgl1} + \tau(A^\dagger b - \widehat x_{\rm spgl1})) - b\|^2\\
 &=\tau^2\|A(A^\dagger b - \widehat x_{\rm spgl1})\|^2 + 2\tau\|A(A^\dagger b - \widehat x_{\rm spgl1})\|\|A(\widehat x_{\rm spgl1} - b\| + \|A(\widehat x_{\rm spgl1} - b\|^2
\end{aligned}
\end{equation*}
which is a quadratic function in $\tau$. One can then obtain a closed form formula for $\tau$.


	\item  \textit{\textbf{Reviewer:}} \textit{Numerical results: As far as I see, Algorithm 3 does not require to find a completely feasible initial point ($x_0\in F$) but just in C ($x_0\in C$). I did not find any statement about how the initial point is chosen for ESQM$_{\rm ls}$. In principle, SPGL1 and Slater point are not needed. Therefore, it is not fair to say that FPA is faster since the sum of the iteration time plus the required initialization exceeds the CPU time of ESQM$_{\rm ls}$. Would it be any other cheaper way to compute $x_0$?}
	
	\textbf{Response:} Thanks for the comment. Following your comment, we explore the effect of the choice of initial points for the two algorithms in solving (6.1). 

Table 1 below shows the computational results when the initial points for the ESQMs are chosen as
$x^0 = (0, 0, \cdots, 0)$. Although ESQM$_{\rm ls}$ becomes faster when the initial point is zero, FPA$_{\rm retract}$ with initial point chosen in (6.6) still seems to be faster than ESQM$_{\rm ls}$ with zero initial point.

{\color{red} TK: Why is FPA also using zero initial point? That does not make any sense. Don't we need feasible starting points?

We only need ONE table, I think: ESQM with zero initial point and our method with our suggested initial point. Please re-do the table.}

\begin{table}[H]
\caption{Computational results for problem (6.2) with initial point $x^0 = (0, 0, \cdots, 0)$ for ESQM}\label{table1}
\begin{center}
{\footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|}\hline
\phantom{\diagbox{Date}{$i$}} & \multicolumn{1}{|c|}{Method} & \multicolumn{1}{|c|}{$i = 2$} & \multicolumn{1}{c|}{ $i = 4$ } & \multicolumn{1}{c|}{ $i = 6$ } & \multicolumn{1}{|c|}{ $i = 8$ } & \multicolumn{1}{c|}{ $i = 10$ }\\\cline{1-7}
\multirow{7}*{CPU time} & \multirow{1}*{QR}
&   0.50 &   3.11 &   9.79 &  22.95 &  43.99\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{Slater}
&   0.01 &   0.02 &   0.05 &   0.09 &   0.15\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{SPGL1}
&   1.38 &   7.54 &  15.69 &  28.50 &  50.25\\\cline{2-7} \multirow{1}*{} & \multirow{1}*{FPA$_{\rm retract}$}
&  14.09 &  62.33 & 139.27 & 241.40 & 388.70\\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
&   4.92 &  25.75 &  59.09 & 102.56 & 170.23\\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
&   4.42 &  21.80 &  49.89 &  86.37 & 142.02\\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
&  13.36 &  59.88 & 135.25 & 236.76 & 378.49\\\cline{1-7} \multirow{4}*{Iter} & \multirow{1}*{FPA$_{\rm retract}$}
&   1638 &   1847 &   1885 &   1883 &   1957\\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
&    759 &   1025 &   1075 &   1080 &   1158\\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
&    767 &    913 &    946 &    941 &    991\\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
&   2439 &   2579 &   2618 &   2620 &   2670\\\cline{1-7} \multirow{5}*{RecErr} & \multirow{1}*{SPGL1}
&  0.060 &  0.051 &  0.055 &  0.055 &  0.046\\\cline{2-7} \multirow{1}*{} & \multirow{1}*{FPA$_{\rm retract}$}
&  0.030 &  0.034 &  0.035 &  0.034 &  0.036\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
&  0.030 &  0.034 &  0.035 &  0.034 &  0.036\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
&  0.030 &  0.034 &  0.035 &  0.034 &  0.036\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
&  0.030 &  0.034 &  0.035 &  0.034 &  0.036\\\cline{1-7} \multirow{5}*{Residual} & \multirow{1}*{SPGL1}
& -1.72e-04 & -2.10e-04 & -1.53e-04 & -1.37e-04 & -1.26e-04\\\cline{2-7} \multirow{1}*{} & \multirow{1}*{FPA$_{\rm retract}$}
& 1.69e-15 & 1.35e-15 & 2.79e-16 & -7.54e-16 & -1.42e-15\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
& 1.21e-10 & 1.22e-10 & 1.12e-10 & 1.10e-10 & 1.11e-10\\\cline{2-2} \multirow{1}*{}      & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
& 1.06e-10 & 9.98e-11 & 9.32e-11 & 9.36e-11 & 9.10e-11\\\cline{2-2} \multirow{1}*{}      & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
& 1.01e-10 & 9.93e-11 & 9.77e-11 & 9.47e-11 & 9.60e-11\\\cline{1-7}
\end{tabular}
}
\end{center}
\end{table}



\begin{table}[H]
\caption{Computational results for problem (6.2) with initial point $x^0$ is given as in (6.6) for all algorithms}\label{table1}
\begin{center}
{\footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|}\hline
\phantom{\diagbox{Date}{$i$}} & \multicolumn{1}{|c|}{Method} & \multicolumn{1}{|c|}{$i = 2$} & \multicolumn{1}{c|}{ $i = 4$ }
& \multicolumn{1}{c|}{ $i = 6$ } & \multicolumn{1}{|c|}{ $i = 8$ } & \multicolumn{1}{c|}{ $i = 10$ }\\\cline{1-7}
\multirow{7}*{CPU time} & \multirow{1}*{QR}
&   0.47 &   3.05 &   9.83 &  23.25 &  44.84\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{Slater}
&   0.01 &   0.02 &   0.05 &   0.09 &   0.15\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{SPGL1}
&   1.59 &   7.35 &  15.20 &  30.17 &  48.54\\\cline{2-7} \multirow{1}*{} & \multirow{1}*{FPA$_{\rm retract}$}
&   3.40 &  12.30 &  37.94 &  56.91 &  97.89\\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
&   5.43 &  23.38 &  61.11 & 104.34 & 173.99\\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
&   4.64 &  20.41 &  51.46 &  88.36 & 146.46\\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
&  13.25 &  58.83 & 136.92 & 239.48 & 382.51\\\cline{1-7} \multirow{4}*{Iter} & \multirow{1}*{FPA$_{\rm retract}$}
&    420 &    368 &    520 &    444 &    493\\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
&    893 &    937 &   1113 &   1099 &   1186\\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
&    839 &    863 &    977 &    963 &   1025\\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
&   2508 &   2550 &   2653 &   2650 &   2702\\\cline{1-7} \multirow{5}*{RecErr} & \multirow{1}*{SPGL1}
&  0.061 &  0.045 &  0.066 &  0.051 &  0.054\\\cline{2-7} \multirow{1}*{} & \multirow{1}*{FPA$_{\rm retract}$}
&  0.032 &  0.032 &  0.035 &  0.035 &  0.037\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
&  0.032 &  0.032 &  0.035 &  0.035 &  0.037\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
&  0.032 &  0.032 &  0.035 &  0.035 &  0.037\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
&  0.032 &  0.032 &  0.035 &  0.035 &  0.037\\\cline{1-7} \multirow{5}*{Residual} & \multirow{1}*{SPGL1}
& -2.72e-04 & -2.20e-04 & -1.41e-04 & -1.19e-04 & -1.30e-04\\\cline{2-7} \multirow{1}*{} & \multirow{1}*{FPA$_{\rm retract}$}
& -1.86e-16 & 2.98e-16 & 1.85e-15 & -5.27e-16 & -1.31e-15\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
& 1.08e-10 & 1.15e-10 & 1.01e-10 & 1.09e-10 & 1.05e-10\\\cline{2-2} \multirow{1}*{}      & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
& 9.49e-11 & 9.50e-11 & 8.90e-11 & 9.60e-11 & 9.95e-11\\\cline{2-2} \multirow{1}*{}      & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
& 9.35e-11 & 9.91e-11 & 9.14e-11 & 9.44e-11 & 9.34e-11\\\cline{1-7}
\end{tabular}
}
\end{center}
\end{table}


    \item  \textit{\textbf{Reviewer:}} \textit{P26: In table 2, the time of SPGL1 for i = 4 is greater than for i = 6. Does it make any sense?}
	
	\textbf{Response:} Thanks for this nice point. We double checked and found that the codes are correct. The results in table 2 are the average based on 30 randomly generating instances. When $i = 4$, it seems that two sets of data take exceptionally more time, so it appears that SPGL1 takes more time in the case $i = 4$ than in the case $i = 6$.

    \item  \textit{\textbf{Reviewer:}} \textit{References: It seems that there exists a more recent version (2016) of [38]. Update [43] and all the referenced results from there to its published version (SIAM J. Optim., 31(3), 2024--2054.).}
	
	\textbf{Response:} Thanks for the comment. We have updated the reference. See reference [43].






\end{enumerate}


\section*{Author's responses to Referee 2}
Thank you very much for reading the manuscript carefully and providing useful suggestions. The following is our response to your suggestions and comments one by one.
%To facilitate the review process, we have marked all our changed in blue.

\paragraph{Main comments:}
\begin{enumerate}
   \item \textit{\textbf{Reviewer:}} {\it The retraction strategy employed by the authors is nothing but an interpolation step, already present in the Support Hyperplane Method (SHM) by Veinott (1967) (see reference [A]). Moreover, in contrast to the setting of the proposed Algorithm 1, the work [A] (and more recently its regularized version in [B]) does not require the constraint function(s) to be convex, but the feasible set. This weaker assumption allows the nonlinear constraint function to have only generalized convexity properties (e.g., quasi-convexity, alpha-convexity [B]). That being said, we believe it is possible to weaken the convexity assumption on the constraint functions in Algorithm 1 with only a few modifications in the convergence analysis. Could the authors confirm that?

       Apart from the interpolation step, what are the main differences between the proposed methodologies and paper [42]?

Although Algorithm 1 employs the SHM's interpolation step, it differs substantially from SHM due to the DC structure. However, Algorithm 1, as well as Algorithm 2, handle the DC structure similarly to the Proximal Linearized Method for DC programming [C,D,E]: at every iteration, the concave part is linearized, and a quadratic term is added to form the objective function in the convex subproblem. In particular, the method in [E] seems more general than Algorithms 1 and 2 because the former allows for DC-constrained DC programs. Therefore, we kindly ask the authors to put their methodology in perspective with [E].

That being said, we see the methodology presented in this manuscript as a "linearized proximal method with support hyperplane for DC programs."
Indeed, the above seems a better title for the work because it connects the core ideas in the manuscript.}

    \textbf{Response:} Thanks for the comment.

    {\it Difference from [A,B]}: We would like to point out that our method is {\it significantly different} from the one in [A,B]. Indeed, [A,B] employs a cutting plane strategy. In particular, as the algorithm progresses, the number of halfspaces involved in the subproblem {\em grows}, with many of them constructed based on information from the \emph{past iterates}. They exploited the fact that compact convex sets can be written as the intersection of (infinitely many) halfspaces, and deduced convergence based on this {\em without} requiring any descent property of the (interpolated) sequence generated.

    In contrast, our method only requires a {\it fixed} number of halfspaces in each iteration, and the constraint functions are linearized only at the \emph{current iterate}. Thus, the computational effort for solving each subproblem is similar. Moreover, we have to sufficiently regularize the objective of our subproblem so that our retraction (or interpolation) step induces a descent: this is an ingredient not present in the method in [A,B] because their convergence is based on the fact that the intersection of the \emph{growing} number of halfspaces constructed in their algorithm converges to the original feasible set.

    {\it Difference from [42]}: Apart from the retraction step, the key difference is that we are using linear approximations to the constraint functions in the subproblems, while the method in [42] is based on quadratic \emph{majorants} of the constraint functions. Thus, one obtains a feasible point by solving the subproblems in the algorithm in [42], but one does not necessarily obtain a feasible point by solving the subproblem in the FPA method: hence, a retraction step kicks in to restore feasibility. Finally, as mentioned in footnote 1 on page 2, our subproblems based on linear approximations are potentially simpler to solve than those quadratically constrained subproblems in [42].

    {\it Difference from [C,D]:} The algorithms proposed in these papers do not seem to use any approximation to the constraint set or exploit the explicit form of the constraint functions.

    {\it Difference from [E]:} The most closely related algorithm is algorithm 2 in [E]. In this algorithm, the DC constraint function is replaced a convex majorant. In spirit, this is similar to what is done in standard DCA (similar to, for example, [42]), and is different from our algorithm as discussed above. Moreover, the algorithm in [E] constructs subproblems by including only constraints that are approximately ``active". This is an ingredient not considered in our algorithm and it is an interesting future research direction as a possible strategy for constructing simpler subproblems for our algorithm.

    Finally, after looking at [A,B] kindly suggested by the reviewer, we feel that the term ``support hyperplane method" typically reminds the readers of cutting plane strategy, which usually involves a {\em growing number of} halfspaces constructed from the current and {\em previous iterates}. Our method, in contrast, involves a {\em fixed} number of halfspaces constructed via linear approximating the constraint functions at the current iterate. Thus, we believe that it is better not to mention supporting hyperplane in the title. Keeping the current title may be more appropriate.

    \item \textit{\textbf{Reviewer:}} {\it The assumptions are poorly stated. For instance, Assumption 3.1 (announced in the key Theorems 3.1 and 3.2) asserts that $g_i$ is convex and a Slater point exists. However, the authors need much more than this: $g_i$ needs to be differentiable (rightly stated in the proofs and other parts), and the Slater point must be known. Note that knowing a Slater point is much more restrictive than only assuming its existence. We kindly ask the authors to fix this issue.}

    \textbf{Response:} Thanks for the comment. We believe that there could be some misunderstanding.

    Indeed, we have made it clear right after stating model (1.1) that there are differentiability assumptions on $g_i$ (which unfortunately went into page 2 and probably got unnoticed). We have been referring to model problem (1.1) in all our assumptions and theorems, which should be sufficient to indicate that we are assuming all conditions stated right after (1.1) for $P_1$, $P_2$, $g_i$ and $C$.

	\item \textit{\textbf{Reviewer:}} \textit{ a) In the Introduction, the authors related their methodology with Sequential Quadratic Programming. However, this link is unclear since this manuscript deals with nonsmooth DC programs, and SQP is for (twice) differentiable problems. Could the authors please give more details on such a connection? Is it related to the sequential DC programming ideas from [F]?}
	
	\textbf{Response:} Thanks for the comment.

    A key common feature between our method and SQP-type method lies in the way the constraint set is dealt with. In both methods, the constraint functions are replaced by linear approximations and feasibility to the original problem is not guaranteed by solving the subproblems. Our key innovation is to further incorporate the ``retract" idea from manifold optimization to create a feasible next iterate.

    As for the sequential DC programming method the reviewer suggested, it appears to us that the constraint set is fixed for all iterations. Thus, it is different from our method which is constructing successive polyhedral approximation to the feasible set.
	
	\item \textit{\textbf{Reviewer:}} \textit{ b) As far as we can tell, Definition 2.3 is about criticality and not stationarity. Stationarity in DC programming is a strong condition, and most algorithms are only ensured to compute critical points. Please see [E] for the differences between these two conditions.}
	
	\textbf{Response:} Thanks for the comment. We agree with this comment. We changed all ``stationary points" to ``critical points". See, for example, Definition 2.3.
	
	\item  \textit{\textbf{Reviewer:}} \textit{ c) Theorem 3.2. Please recall that $u^k = \tilde u$.}
	
	\textbf{Response:} Thanks for the comment. We recalled that the sequence $\{u^k\}$ is generated by Algorithm 1 in the statement of the theorem. A similar modification is done in Theorem 4.2.

	\item \textit{\textbf{Reviewer:}} \textit{ d) Assumption 4.1. The notation $\xi$ has already been employed for the subgradient of $P_2$.}

    \textbf{Response:} Thanks for the comment. We replace $\xi$ with $\varphi$ in Assumption 4.1 and beyond.

	\item \textit{\textbf{Reviewer:}} \textit{ e) Assumption 4.2. The Slater point must be known.}

    \textbf{Response:} Thanks for the comment. We definitely agree that the Slater point must be known. We modified Assumption 3.1 and 4.2.

	\item \textit{\textbf{Reviewer:}} \textit{ f) Subproblem (4.6): Should it be $\sigma^x$?}

    \textbf{Response:} Thanks for the comment. It should be $\sigma$ in subproblem (4.6). Indeed, in view of Lemma 4.1(iv), one can see that the inequality constraint in (4.6) is the same as
    \[
    \ell^{x^k}(Ax^k - b) + \langle A^T\nabla \ell^{x^k}(Ax^k - b),x-x^k\rangle \le \tilde \sigma^{x^k}.
    \]

	\item \textit{\textbf{Reviewer:}} \textit{ g) Please compare the algorithms with the method presented in [42], by the third author.}

    \textbf{Response:} Thanks for the comment. The SCP was applied in [42] for solving (6.4):
    \begin{equation}\label{pr1}
       \begin{array}{rl}
        \min\limits_{x\in\R^n} & \|x\|_1 - \mu\|x\| \\
        {\rm s.t.} & \|Ax - b\|_{LL_2,\gamma}\leq \overline{\sigma},
       \end{array}
     \end{equation}
    We use the same parameter settings as in [42] for SCP. The computational results are presented in Table 3.

\begin{table}[h]
\caption{Computational results for problem \eqref{pr1} with initial point $x^0$ is given as in (6.6)}\label{table3}
\begin{center}
{\footnotesize
\begin{tabular}{|c|c|c|c|c|c|c|}\hline
\phantom{\diagbox{Date}{$i$}} & \multicolumn{1}{|c|}{Method} & \multicolumn{1}{|c|}{$i = 2$} & \multicolumn{1}{c|}{ $i = 4$ }
& \multicolumn{1}{c|}{ $i = 6$ } & \multicolumn{1}{|c|}{ $i = 8$ } & \multicolumn{1}{c|}{ $i = 10$ }\\\cline{1-7}
\multirow{8}*{CPU time} & \multirow{1}*{QR}
&   0.56 &   3.17 &  10.04 &  23.39 &  45.84\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{Slater}
&   0.01 &   0.02 &   0.06 &   0.10 &   0.15\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{SPGL1}
&   3.24 &  13.12 &  34.00 & 205.99 & 233.59\\\cline{2-7} \multirow{1}*{} & \multirow{1}*{SCP}
&   2.67 &  14.19 &  19.85 &  39.25 &  58.30\\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{FPA$_{\rm retract}$}
&   4.57 & 191.87 &  40.39 & 128.89 & 160.03\\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
&  17.38 &  66.68 & 123.05 & 211.69 & 179.34\\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
&  16.54 &  65.41 & 137.92 & 217.52 & 294.88\\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
&  18.35 &  82.69 & 195.55 & 329.15 & 397.47\\\cline{1-7} \multirow{5}*{Iter} & \multirow{1}*{SCP}
&    200 &    325 &    203 &    245 &    237\\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{FPA$_{\rm retract}$}
&    577 &   6069 &    567 &   1072 &    859\\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
&   2590 &   2629 &   2204 &   2256 &   1236\\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
&   2479 &   2578 &   2470 &   2315 &   2026\\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
&   2766 &   3266 &   3505 &   3506 &   2741\\\cline{1-7} \multirow{6}*{RecErr} & \multirow{1}*{SPGL1}
&  1.699 &  3.423 &  1.826 &  2.630 &  2.465\\\cline{2-7} \multirow{1}*{} & \multirow{1}*{SCP}
&  0.071 &  0.073 &  0.075 &  0.074 &  0.075\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{FPA$_{\rm retract}$}
&  0.071 &  0.073 &  0.075 &  0.074 &  0.075\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
&  0.071 &  0.073 &  0.075 &  0.074 &  0.075\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
&  0.071 &  0.073 &  0.075 &  0.074 &  0.075\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
&  0.071 &  0.073 &  0.075 &  0.074 &  0.075\\\cline{1-7} \multirow{6}*{Residual} & \multirow{1}*{SPGL1}
& -5.45e-01 & -5.84e-01 & -7.09e-01 & -7.08e-01 & -7.38e-01\\\cline{2-7} \multirow{1}*{} & \multirow{1}*{SCP}
& -1.31e-10 & -1.53e-10 & -1.45e-10 & -1.78e-10 & -1.60e-10\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{FPA$_{\rm retract}$}
& -1.25e-12 & -1.37e-12 & -1.18e-12 & -1.22e-12 & -1.07e-12\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
& 1.84e-11 & 1.94e-11 & 3.42e-11 & 4.38e-11 & 9.14e-11\\\cline{2-2} \multirow{1}*{}      & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
& 2.62e-11 & 2.35e-11 & 2.58e-11 & 2.87e-11 & 2.32e-11\\\cline{2-2} \multirow{1}*{}      & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
& 1.84e-11 & 1.52e-11 & 1.24e-11 & 1.41e-11 & 2.30e-11\\\cline{1-7}
\end{tabular}
}
\end{center}
\end{table}

From Table 3, one can see that the recovery errors of the algorithms are comparable, and SCP is usually faster. This is likely because SCP can take advantage of BB step size (and hence some sort of second-order information), but currently we still do not know how to take advantage of second-order information suitably in our scheme.

From the table above, it appears that our proposed method is not the method of choice when it comes to solving \eqref{pr1}.
Note that although the subproblems of SCP for \eqref{pr1} involve nonlinear constraints, thanks to the polyhedral nature of $\ell_1$ norm, they can still be solved efficiently via some root-finding procedure; see the appendix of [43]. This prompts us to consider some other objective functions where the corresponding SCP subproblems are not easy while the subproblems for our method (which are linearly constrained) can still be solved efficiently using Newton's method; this setting would be closer to our original motivation, as stated in footnote 1 of the manuscript.

One natural scenario arises from compressed sensing problems for complex signals with Cauchy noise, which results in the following problem that involves group-LASSO type objective. The derivation is detailed in the new Section 6.2.
\begin{equation}\label{E2}
  \begin{array}{rl}
    \min\limits_{x\in\R^n} & \sum\limits_{J\in\mathcal{J}}\|x_J\| - \mu\|x\| \\
    {\rm s.t.} & \|Ax - b\|_{LL_2,\gamma}\leq \overline{\sigma},
  \end{array}
\end{equation}
While theoretically one can apply SCP to the above problem, it is unclear to us how the subproblems, which are nonlinearly constrained, can be solved efficiently. In contrast, the subproblems that arise in our approach can still be solved efficiently using Newton's method. We have now adopted this complex version of compressed sensing problems in our numerical experiment section. 
%Notice that this problem can not be solved efficiently by SCP in [42] (as we don't know how to efficiently solve the subproblem of SCP in this case). Hence we replace problem (6.3) in article with problem \eqref{E2}, and have modified it accordingly. We compare FPA$_{\rm retract}$ and ESQM$_{\rm ls}$ on solving random instances of \eqref{E2}. The results are shown in Table 4 and Table 5, which correspond to different initial points.
%
%\begin{table}[H]
%\caption{Computational results for problem \eqref{E2} with initial point $x^0$ is given as in (6.6)}\label{table3}
%\begin{center}
%{\footnotesize
%\begin{tabular}{|c|c|c|c|c|c|c|}\hline
%\phantom{\diagbox{Date}{$i$}} & \multicolumn{1}{|c|}{Method} & \multicolumn{1}{|c|}{$i = 2$} & \multicolumn{1}{c|}{ $i = 4$ }
%& \multicolumn{1}{c|}{ $i = 6$ } & \multicolumn{1}{|c|}{ $i = 8$ } & \multicolumn{1}{c|}{ $i = 10$ }\\\cline{1-7}
%\multirow{7}*{CPU time} & \multirow{1}*{QR}
%&   0.63 &   4.27 &  11.15 &  24.66 &  51.66\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{Slater}
%&   0.01 &   0.03 &   0.05 &   0.10 &   0.15\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{SPGL1}
%&   1.18 &  11.16 &  34.97 &  43.14 &  95.33\\\cline{2-7} \multirow{1}*{} & \multirow{1}*{FPA$_{\rm retract}$}
%&   3.64 &  15.43 &  34.70 &  46.92 &  88.78\\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
%&  13.62 &  34.03 &  41.46 &  59.36 &  57.94\\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
%&  15.86 &  47.38 &  57.92 &  80.34 & 132.93\\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
%&  17.33 &  59.84 & 111.44 & 181.45 & 208.90\\\cline{1-7} \multirow{4}*{Iter} & \multirow{1}*{FPA$_{\rm retract}$}
%&    224 &    363 &    410 &    340 &    424\\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
%&   1423 &   1125 &    670 &    570 &    366\\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
%&   1655 &   1563 &    939 &    787 &    866\\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
%&   1820 &   2001 &   1841 &   1832 &   1365\\\cline{1-7} \multirow{5}*{RecErr} & \multirow{1}*{SPGL1}
%&  0.867 &  1.490 &  1.806 &  1.360 &  1.768\\\cline{2-7} \multirow{1}*{} & \multirow{1}*{FPA$_{\rm retract}$}
%&  0.048 &  0.051 &  0.052 &  0.052 &  0.052\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
%&  0.048 &  0.051 &  0.052 &  0.052 &  0.052\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
%&  0.048 &  0.051 &  0.052 &  0.052 &  0.052\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
%&  0.048 &  0.051 &  0.052 &  0.052 &  0.052\\\cline{1-7} \multirow{5}*{Residual} & \multirow{1}*{SPGL1}
%& -5.23e-01 & -6.27e-01 & -6.81e-01 & -6.56e-01 & -7.32e-01\\\cline{2-7} \multirow{1}*{} & \multirow{1}*{FPA$_{\rm retract}$}
%& -2.88e-12 & -2.41e-12 & -2.42e-12 & -2.41e-12 & -2.17e-12\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
%& 3.88e-11 & 6.21e-11 & 1.42e-10 & 1.16e-10 & 2.90e-10\\\cline{2-2} \multirow{1}*{}      & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
%& 4.15e-11 & 6.45e-11 & 1.26e-10 & 6.73e-11 & 6.65e-11\\\cline{2-2} \multirow{1}*{}      & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
%& 3.11e-11 & 2.71e-11 & 3.73e-11 & 4.77e-11 & 7.86e-11\\\cline{1-7}
%\end{tabular}
%}
%\end{center}
%\end{table}
%
%
%
%\begin{table}[H]
%\caption{Computational results for problem \eqref{E2} with initial point $x^0= (0, 0, \cdots, 0)$}\label{table3}
%\begin{center}
%{\footnotesize
%\begin{tabular}{|c|c|c|c|c|c|c|}\hline
%\phantom{\diagbox{Date}{$i$}} & \multicolumn{1}{|c|}{Method} & \multicolumn{1}{|c|}{$i = 2$} & \multicolumn{1}{c|}{ $i = 4$ }
%& \multicolumn{1}{c|}{ $i = 6$ } & \multicolumn{1}{|c|}{ $i = 8$ } & \multicolumn{1}{c|}{ $i = 10$ }\\\cline{1-7}
%\multirow{7}*{CPU time} & \multirow{1}*{QR}
%&   0.61 &   4.14 &  10.99 &  40.10 &  58.79\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{Slater}
%&   0.01 &   0.03 &   0.05 &   0.15 &   0.16\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{SPGL1}
%&   1.15 &  11.15 &  34.89 & 757.10 &  95.79\\\cline{2-7} \multirow{1}*{} & \multirow{1}*{FPA$_{\rm retract}$}
%&   5.05 &  19.66 &  41.70 &  69.97 &  98.92\\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
%&  13.02 &  35.24 &  51.35 &  92.84 &  76.91\\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
%&  15.68 &  46.91 &  64.06 & 123.14 & 152.65\\\cline{2-2} \multirow{1}*{}  & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
%&  19.24 &  64.05 & 122.80 & 238.13 & 231.80\\\cline{1-7} \multirow{4}*{Iter} & \multirow{1}*{FPA$_{\rm retract}$}
%&    320 &    440 &    496 &    381 &    469\\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
%&   1353 &   1152 &    836 &    672 &    487\\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
%&   1652 &   1555 &   1045 &    989 &    984\\\cline{2-2} \multirow{1}*{}     & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
%&   2055 &   2112 &   2048 &   1942 &   1498\\\cline{1-7} \multirow{5}*{RecErr} & \multirow{1}*{SPGL1}
%&  0.867 &  1.490 &  1.806 &  1.360 &  1.768\\\cline{2-7} \multirow{1}*{} & \multirow{1}*{FPA$_{\rm retract}$}
%&  0.048 &  0.051 &  0.052 &  0.052 &  0.052\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
%&  0.048 &  0.051 &  0.052 &  0.052 &  0.052\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
%&  0.048 &  0.051 &  0.052 &  0.052 &  0.052\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
%&  0.048 &  0.051 &  0.052 &  0.052 &  0.052\\\cline{1-7} \multirow{5}*{Residual} & \multirow{1}*{SPGL1}
%& -5.23e-01 & -6.27e-01 & -6.81e-01 & -6.56e-01 & -7.32e-01\\\cline{2-7} \multirow{1}*{} & \multirow{1}*{FPA$_{\rm retract}$}
%& -2.82e-12 & -2.47e-12 & -2.45e-12 & -2.52e-12 & -2.20e-12\\\cline{2-2} \multirow{1}*{} & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.5}$}
%& 6.59e-11 & 4.79e-11 & 2.57e-10 & 1.05e-10 & 2.68e-10\\\cline{2-2} \multirow{1}*{}      & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.1}$}
%& 5.94e-11 & 6.38e-11 & 8.28e-11 & 8.97e-11 & 9.09e-11\\\cline{2-2} \multirow{1}*{}      & \multirow{1}*{ESQM$_{{\rm ls},\ \delta = 0.02}$}
%& 3.14e-11 & 3.01e-11 & 3.56e-11 & 4.44e-11 & 8.38e-11\\\cline{1-7}
%\end{tabular}
%}
%\end{center}
%\end{table}


\end{enumerate}


\end{document}
